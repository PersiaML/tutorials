<!DOCTYPE HTML>
<html lang="en" class="sidebar-visible no-js light">
    <head>
        <!-- Book generated using mdBook -->
        <meta charset="UTF-8">
        <title>PersiaML Tutorials</title>
        <meta name="robots" content="noindex" />
        <!-- Custom HTML head -->
        

        <meta content="text/html; charset=utf-8" http-equiv="Content-Type">
        <meta name="description" content="Guide on how to use PersiaML, a large scale sparse model training framework">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="theme-color" content="#ffffff" />

        <link rel="icon" href="favicon.svg">
        <link rel="shortcut icon" href="favicon.png">
        <link rel="stylesheet" href="css/variables.css">
        <link rel="stylesheet" href="css/general.css">
        <link rel="stylesheet" href="css/chrome.css">
        <link rel="stylesheet" href="css/print.css" media="print">
        <!-- Fonts -->
        <link rel="stylesheet" href="FontAwesome/css/font-awesome.css">
        <link rel="stylesheet" href="fonts/fonts.css">
        <!-- Highlight.js Stylesheets -->
        <link rel="stylesheet" href="highlight.css">
        <link rel="stylesheet" href="tomorrow-night.css">
        <link rel="stylesheet" href="ayu-highlight.css">

        <!-- Custom theme stylesheets -->
        <link rel="stylesheet" href="open-in.css">
    </head>
    <body>
        <!-- Provide site root to javascript -->
        <script type="text/javascript">
            var path_to_root = "";
            var default_theme = window.matchMedia("(prefers-color-scheme: dark)").matches ? "navy" : "light";
        </script>

        <!-- Work around some values being stored in localStorage wrapped in quotes -->
        <script type="text/javascript">
            try {
                var theme = localStorage.getItem('mdbook-theme');
                var sidebar = localStorage.getItem('mdbook-sidebar');

                if (theme.startsWith('"') && theme.endsWith('"')) {
                    localStorage.setItem('mdbook-theme', theme.slice(1, theme.length - 1));
                }

                if (sidebar.startsWith('"') && sidebar.endsWith('"')) {
                    localStorage.setItem('mdbook-sidebar', sidebar.slice(1, sidebar.length - 1));
                }
            } catch (e) { }
        </script>

        <!-- Set the theme before any content is loaded, prevents flash -->
        <script type="text/javascript">
            var theme;
            try { theme = localStorage.getItem('mdbook-theme'); } catch(e) { }
            if (theme === null || theme === undefined) { theme = default_theme; }
            var html = document.querySelector('html');
            html.classList.remove('no-js')
            html.classList.remove('light')
            html.classList.add(theme);
            html.classList.add('js');
        </script>

        <!-- Hide / unhide sidebar before it is displayed -->
        <script type="text/javascript">
            var html = document.querySelector('html');
            var sidebar = 'hidden';
            if (document.body.clientWidth >= 1080) {
                try { sidebar = localStorage.getItem('mdbook-sidebar'); } catch(e) { }
                sidebar = sidebar || 'visible';
            }
            html.classList.remove('sidebar-visible');
            html.classList.add("sidebar-" + sidebar);
        </script>

        <nav id="sidebar" class="sidebar" aria-label="Table of contents">
            <div class="sidebar-scrollbox">
                <ol class="chapter"><li class="chapter-item expanded "><a href="introduction.html"><strong aria-hidden="true">1.</strong> Introduction</a></li><li class="chapter-item expanded "><a href="installation/index.html"><strong aria-hidden="true">2.</strong> Installation</a></li><li class="chapter-item expanded "><a href="getting-started/index.html"><strong aria-hidden="true">3.</strong> Getting Started</a></li><li class="chapter-item expanded "><a href="benchmark/index.html"><strong aria-hidden="true">4.</strong> Benchmark</a></li><li class="chapter-item expanded "><a href="data-processing/index.html"><strong aria-hidden="true">5.</strong> Data Processing</a></li><li class="chapter-item expanded "><a href="monitoring/index.html"><strong aria-hidden="true">6.</strong> Monitoring</a></li><li class="chapter-item expanded "><a href="inference/index.html"><strong aria-hidden="true">7.</strong> Inference</a></li><li class="chapter-item expanded "><a href="model-checkpointing/index.html"><strong aria-hidden="true">8.</strong> Model Checkpointing</a></li><li class="chapter-item expanded "><a href="configuration/index.html"><strong aria-hidden="true">9.</strong> Configuration</a></li><li class="chapter-item expanded "><a href="troubleshooting/index.html"><strong aria-hidden="true">10.</strong> Troubleshooting</a></li><li class="chapter-item expanded "><a href="kubernetes-integration/index.html"><strong aria-hidden="true">11.</strong> Kubernetes Integration</a></li></ol>
            </div>
            <div id="sidebar-resize-handle" class="sidebar-resize-handle"></div>
        </nav>

        <div id="page-wrapper" class="page-wrapper">

            <div class="page">
                
                <div id="menu-bar-hover-placeholder"></div>
                <div id="menu-bar" class="menu-bar sticky bordered">
                    <div class="left-buttons">
                        <button id="sidebar-toggle" class="icon-button" type="button" title="Toggle Table of Contents" aria-label="Toggle Table of Contents" aria-controls="sidebar">
                            <i class="fa fa-bars"></i>
                        </button>
                        <button id="theme-toggle" class="icon-button" type="button" title="Change theme" aria-label="Change theme" aria-haspopup="true" aria-expanded="false" aria-controls="theme-list">
                            <i class="fa fa-paint-brush"></i>
                        </button>
                        <ul id="theme-list" class="theme-popup" aria-label="Themes" role="menu">
                            <li role="none"><button role="menuitem" class="theme" id="light">Light (default)</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="rust">Rust</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="coal">Coal</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="navy">Navy</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="ayu">Ayu</button></li>
                        </ul>
                        <button id="search-toggle" class="icon-button" type="button" title="Search. (Shortkey: s)" aria-label="Toggle Searchbar" aria-expanded="false" aria-keyshortcuts="S" aria-controls="searchbar">
                            <i class="fa fa-search"></i>
                        </button>
                    </div>

                    <h1 class="menu-title">PersiaML Tutorials</h1>

                    <div class="right-buttons">
                        <a href="print.html" title="Print this book" aria-label="Print this book">
                            <i id="print-button" class="fa fa-print"></i>
                        </a>
                        <a href="https://github.com/PersiaML/tutorials" title="Git repository" aria-label="Git repository">
                            <i id="git-repository-button" class="fa fa-github"></i>
                        </a>
                    </div>
                </div>

                <div id="search-wrapper" class="hidden">
                    <form id="searchbar-outer" class="searchbar-outer">
                        <input type="search" id="searchbar" name="searchbar" placeholder="Search this book ..." aria-controls="searchresults-outer" aria-describedby="searchresults-header">
                    </form>
                    <div id="searchresults-outer" class="searchresults-outer hidden">
                        <div id="searchresults-header" class="searchresults-header"></div>
                        <ul id="searchresults">
                        </ul>
                    </div>
                </div>
                <!-- Apply ARIA attributes after the sidebar and the sidebar toggle button are added to the DOM -->
                <script type="text/javascript">
                    document.getElementById('sidebar-toggle').setAttribute('aria-expanded', sidebar === 'visible');
                    document.getElementById('sidebar').setAttribute('aria-hidden', sidebar !== 'visible');
                    Array.from(document.querySelectorAll('#sidebar a')).forEach(function(link) {
                        link.setAttribute('tabIndex', sidebar === 'visible' ? 0 : -1);
                    });
                </script>

                <div id="content" class="content">
                    <main>
                        <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css" integrity="sha384-AfEj0r4/OFrOo5t7NnNe46zW/tFgW6x/bCJG8FqQCEo3+Aro6EYUG4+cU+KJWu/X" crossorigin="anonymous">
<h1 id="introduction"><a class="header" href="#introduction">Introduction</a></h1>
<footer id="open-on-gh">Found a bug? <a href="https://github.com/PersiaML/tutorials/edit/main/src/introduction.md">Edit this file on GitHub.</a></footer><div style="break-before: page; page-break-before: always;"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css" integrity="sha384-AfEj0r4/OFrOo5t7NnNe46zW/tFgW6x/bCJG8FqQCEo3+Aro6EYUG4+cU+KJWu/X" crossorigin="anonymous">
<h1 id="installation"><a class="header" href="#installation">Installation</a></h1>
<ul>
<li><a href="installation/index.html#use-docker-images">Use Docker Images </a>
<ul>
<li><a href="installation/index.html#using-pre-built-images">Using pre-built images</a></li>
<li><a href="installation/index.html#building-the-image-locally">Building the image locally</a></li>
</ul>
</li>
<li><a href="installation/index.html#install-manually">Install Manually</a>
<ul>
<li><a href="installation/index.html#common-requirements">Common Requirements</a></li>
<li><a href="installation/index.html#install-from-pip">Install from Pip</a></li>
<li><a href="installation/index.html#install-from-source">Install from source</a></li>
</ul>
</li>
</ul>
<h2 id="use-docker-images"><a class="header" href="#use-docker-images">Use Docker Images</a></h2>
<p>The fastest way to training your first Persia task is using pre-built docker images.</p>
<h3 id="using-pre-built-images"><a class="header" href="#using-pre-built-images">Using pre-built images</a></h3>
<pre><code class="language-bash">docker pull persiaml/persia-cuda-runtime:latest
</code></pre>
<h3 id="building-the-image-locally"><a class="header" href="#building-the-image-locally">Building the image locally</a></h3>
<pre><code class="language-bash">git clone git@github.com:PersiaML/PersiaML.git 
# docker image name: persiaml/persia-cuda-runtime:dev
cd PersiaML &amp;&amp; make build_dev_image 
</code></pre>
<h2 id="install-manually"><a class="header" href="#install-manually">Install Manually</a></h2>
<p>You can also install PersiaML manually on your existing system.</p>
<h3 id="common-requirements"><a class="header" href="#common-requirements">Common requirements</a></h3>
<pre><code class="language-bash">export RUSTUP_HOME=/rust
export CARGO_HOME=/cargo
export PATH=/cargo/bin:/rust/bin:PATH

curl -sSf https://sh.rustup.rs | sh -s -- --default-toolchain nightly -y --profile default --no-modify-path
rustup install nightly-2021-06-01

sudo apt-get install -y python3.7 libpython3.7-dev
</code></pre>
<h3 id="install-from-pip"><a class="header" href="#install-from-pip">Install From Pip</a></h3>
<pre><code class="language-bash">USE_CUDA=1 pip3 install persia
</code></pre>
<h3 id="install-from-source"><a class="header" href="#install-from-source">Install From Source</a></h3>
<pre><code class="language-bash">git clone git@github.com:PersiaML/PersiaML.git 
pip3 install click colorlog torch colorama setuptools setuptools-rust setuptools_scm

cd PersiaML
# install cpu version
python3 setup.py install
# install cuda version
USE_CUDA=1 python3 setup.py install
</code></pre>
<footer id="open-on-gh">Found a bug? <a href="https://github.com/PersiaML/tutorials/edit/main/src/installation/index.md">Edit this file on GitHub.</a></footer><div style="break-before: page; page-break-before: always;"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css" integrity="sha384-AfEj0r4/OFrOo5t7NnNe46zW/tFgW6x/bCJG8FqQCEo3+Aro6EYUG4+cU+KJWu/X" crossorigin="anonymous">
<h1 id="getting-started"><a class="header" href="#getting-started">Getting Started</a></h1>
<p>According to DLRM example to make you construct machine learning application based on Persia swiftly</p>
<h2 id="setup"><a class="header" href="#setup">Setup</a></h2>
<ol>
<li>Download the Kaggle Display Advertising Challenge dataset for DLRM example
<pre><code class="language-bash">cd examples/DLRM/data  
curl -L -o data.tar.gz https://ndownloader.figshare.com/files/10082655
# wget https://ndownloader.figshare.com/files/10082655 -o data.tar.gz
tar -zxvf data.tar.gz &amp;&amp; rm -rf data.tar.gz
</code></pre>
</li>
<li>Initialize the persia-core git submodule <code>git submodule init --udpate</code></li>
<li>Prepare for runtime docker image
<pre><code class="language-bash">make build_all 
</code></pre>
</li>
</ol>
<h2 id="process-training-data"><a class="header" href="#process-training-data">Process training data</a></h2>
<p>Kaggle Display Advertising Challenge dataset contain two parts of data that calls <code>dense</code> and <code>sparse</code>.  <code>Dense data</code> is represent by a 1D vector that come from a set of statistics data or extract by <code>DNN model</code> extract from image, video, audio or etc. Dense data should have same dimension for each sample. <code>Sparse data</code> also represents by a 1D vector but the dimension could be various for each sample. <code>Sparse data</code> is a list of category data, for example age, gender, user_id, book_id or etc.PerisaML framework converted <code>sparse data</code> to fixed size <code>dense tensor(1d or 2d)</code> by the process called <code>embedding lookup</code>. </p>
<h3 id="data-preprocess"><a class="header" href="#data-preprocess">Data preprocess</a></h3>
<p>Process Kaggle Display Advertising Challenge raw dataset to <code>numpy.ndarray</code> by <code>numpy</code></p>
<pre><code class="language-python">import numpy as np

&quot;&quot;&quot;
sample of train.txt
0       1       1       5       0       1382    4       15      2       181     1       2               2       68fd1e64      80e26c9b        fb936136        7b4723c4        25c83c98        7e0ccccf        de7995b8        1f89b562     a73ee510 a8cd5504        b2cb9c98        37c9c164        2824a5f6        1adce6ef        8ba8b39a        891b62e7     e5ba7672 f54016b9        21ddcdc9        b1252a9d        07b5194c                3a171ecb        c5c50484        e8b83407        9727dd16

&quot;&quot;&quot;
source_data = &quot;train.txt&quot;
batch_size = 5
batch_data = []

with open(source_data, &quot;r&quot;) as file:
    for line in file:
        splitter = denseline.split(&quot;\t&quot;)
        target = np.int32(line[0])
        dense_sample = np.array(splitter[1:14], dtype=np.float32)
        sparse_sample = np.array(
            list(map(lambda x: int(x, 16), line[14:])), dtype=np.uint64
        ) 
        batch_data.append((dense_sample, sample_sample, target))
        if len(batch_data) == batch_size:
            # process the below
            ...
</code></pre>
<h3 id="persia-data-structure-to-store-training-data"><a class="header" href="#persia-data-structure-to-store-training-data">Persia data structure to store training data</a></h3>
<p>PersiaML provide specific data structure for sparse training scence. The structure can add multiple dense, sparse and target data. </p>
<pre><code class="language-python">from persia.prelude import PyPersiaBatchData

persia_batch_data = PyPersiaBatchData()
batch_dense_data, batch_all_sparse_data, batch_target_data = zip(*batch_data)
</code></pre>
<h3 id="add-dense-data"><a class="header" href="#add-dense-data">Add dense data</a></h3>
<p><code>PyPersiaBatchData</code> provide the <code>add_dense</code> function to add a list of 2d float32 numpy array. It also provide some specific functions to add dense data in various datatype such as <code>add_dense_f32</code>, <code>add_dense_i32</code>, <code>add_dense_f64</code>, <code>add_dense_i64</code>. </p>
<pre><code class="language-python">persia_batch_data = PyPersiaBatchData()
batch_dense_data = np.stack(batch_dense_data)

persia_batch_data.add_dense_f32(batch_dense_data)
</code></pre>
<h3 id="add-sparse-data"><a class="header" href="#add-sparse-data">Add sparse data</a></h3>
<p>Add multiple categories sparse data into <code>PyPersiaBatchData</code>, each category data should have the same batch size and a unique namespace to share feature space. </p>
<pre><code class="language-python">&quot;&quot;&quot;
    In Kaggle Display Advertising Challenge dataset, every categories only lookup one sparse id in each sample. 
    sample below:

    batch_size = 5
    sparse_feature1 = [
        np.array([0]),
        np.array([1]),
        np.array([2]),
        np.array([3])
        np.array([5])
    ]

    sparse_feature2 = [
        np.array([2]),
        np.array([3]),
        np.array([4]),
        np.array([2])
        np.array([6])
    ]
&quot;&quot;&quot;
batch_sparse_category_data = []
for (idx, batch_sparse_data) in enumerate(batch_all_sparse_data):
    category_name = f&quot;sparse_feature{idx}&quot;
    sparse_array = []
    for i in range(batch_size):
        sparse_array.append(batch_sparse_data[i:i+1])
    batch_sparse_category_data.append((category_name, sparse_array))
persia_batch_data.add_sparse(batch_sparse_category_data)
</code></pre>
<h3 id="add-target-data"><a class="header" href="#add-target-data">Add target data</a></h3>
<p>Target data (ground truth) is define as a 2d float32 numpy array, user can add multiple target data into <code>PyPersiaBatchData</code> for multi-task Learning.</p>
<pre><code class="language-python">batch_target_data = np.array(batch_target_data, dtype=np.float32)
batch_target_data = np.stack([batch_target_data[i] for i in range(batch_size)]) 
persia_batch_data.add_target(batch_target_data) # can invoke multiple times for multi task training
</code></pre>
<h3 id="data-transfer"><a class="header" href="#data-transfer">Data transfer</a></h3>
<p>Init the <code>persia_backend</code> to transfer the <code>persia batch data</code> to the persia-middleware and persia-trainer. </p>
<pre><code class="language-python">from persia.ctx import DataCtx
with DataCtx():
    ctx.send_data(persia_batch_data) # register sparse data and transfer remain part to trainer service
</code></pre>
<p><em>review DLRM datacompose codebase at examples/DLRM/data_compose.py</em></p>
<h2 id="define-model"><a class="header" href="#define-model">Define model</a></h2>
<p>Construct the DLRM model after finished the data definition. The Model forward entry receive dense tensors and sparse tensors.</p>
<pre><code class="language-python">from typing import List

import torch
class DLRM(nn.Module):
    def __init__(self, ln, sigmoid_layer):
        ...

    def forward(self, dense: List[torch.Tensor], sparse: List[torch.Tensor]):
        ...

model = DLRM()
</code></pre>
<p><em>review DLRM model codebase at examples/DLRM/model.py</em></p>
<h2 id="define-optimizer"><a class="header" href="#define-optimizer">Define optimizer</a></h2>
<p>Define optimizer in sparse training is as simple as normal torch training. Dense optimizer is define for update the <code>DNN model</code>.Sparse optimizer is define for update the sparse sparse embedding.<code>perisa.sparse.optim</code> provide common optimizer for different training scences.</p>
<pre><code class="language-python">from torch.optim import SGD
from persia.sparse.optim import Adagrad

# DENSE parameters optimizer
dense_optimizer = SGD(model.parameters(), lr=0.1)
# Sparse embedding optimizer
sparse_optimizer = Adagrad(lr=1e-3)
</code></pre>
<h2 id="create-training-context"><a class="header" href="#create-training-context">Create training context</a></h2>
<p>Finally step is create the training context to acquire dataloder and sparse embedding process</p>
<pre><code class="language-python">from persia.ctx import TrainCtx

scaler = torch.cuda.amp.GradScaler() # for half training

with TrainCtx(
    model=model,
    sparse_optimizer=sparse_optimizer,
    dense_optimizer=dense_optimizer,
    device_id=device_id,
) as ctx:
    for (batch_idx, data) in enumerate(ctx.data_loader()):
        (dense, sparse), target = ctx.prepare_features(data)
        output, aux_loss = model((dense, sparse))
        bce_loss = loss_fn(output, target)
        loss = aux_loss + bce_loss
        scaled_loss = ctx.backward(loss)
        logger.info(f&quot;current idx: {batch_idx} loss: {loss}&quot;)

</code></pre>
<p><em>review DLRM model codebase at examples/DLRM/train.py</em></p>
<h2 id="start-training"><a class="header" href="#start-training">Start training</a></h2>
<pre><code class="language-bash">cd examples/DLRM/ &amp;&amp; make run 
# run make stop to remove docker stack job
# make stop
</code></pre>
<footer id="open-on-gh">Found a bug? <a href="https://github.com/PersiaML/tutorials/edit/main/src/getting-started/index.md">Edit this file on GitHub.</a></footer><div style="break-before: page; page-break-before: always;"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css" integrity="sha384-AfEj0r4/OFrOo5t7NnNe46zW/tFgW6x/bCJG8FqQCEo3+Aro6EYUG4+cU+KJWu/X" crossorigin="anonymous">
<h1 id="benchmark"><a class="header" href="#benchmark">Benchmark</a></h1>
<footer id="open-on-gh">Found a bug? <a href="https://github.com/PersiaML/tutorials/edit/main/src/benchmark/index.md">Edit this file on GitHub.</a></footer><div style="break-before: page; page-break-before: always;"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css" integrity="sha384-AfEj0r4/OFrOo5t7NnNe46zW/tFgW6x/bCJG8FqQCEo3+Aro6EYUG4+cU+KJWu/X" crossorigin="anonymous">
<h1 id="data-processing"><a class="header" href="#data-processing">Data Processing</a></h1>
<p>Training data in PersiaML consists of three parts, contiguous data (dense), categorical data (sparse) and label data (target). When training with Persia, first format the original training data into the corresponding Persia data format, and then add them to <code>persia.prelude.PyPersiaBatchData</code>.</p>
<h2 id="contiguous-data"><a class="header" href="#contiguous-data">Contiguous Data</a></h2>
<p>We define the <em>Contiguous Data</em> as <em>Dense Data</em> in our library. Mixed datatypes are supported. One can add multiple 2D <em>Dense Data</em> of different datatypes to <code>PyPersiaBatchData</code> by invoking the corresponding methods. Note that the shape of all 2D Dense data should be equal. (TODO: why?)</p>
<p><em>datatypes <code>PyPersiaBatchData</code> currently support</em></p>
<ul>
<li>add_dense_f32 =&gt; np.float32 </li>
<li>add_dense_f64 =&gt; np.float64</li>
<li>add_dense_i32 =&gt;  np.int32</li>
<li>add_dense_i64 =&gt; np.int64</li>
</ul>
<p><em>code example</em></p>
<pre><code class="language-python">import numpy as np

from persia.prelude import PyPersiaBatchData

batch_data = PyPersiaBatchData()

batch_size = 1024
dim = 256

batch_data.add_dense_f32(np.ones((batch_size, dim), dtype=np.float32))
batch_data.add_dense_i32(np.ones((batch_size, dim), dtype=np.int32))
batch_data.add_dense_f64(np.ones((batch_size, dim), dtype=np.float64))
batch_data.add_dense_i64(np.ones((batch_size, dim), dtype=np.int64))
</code></pre>
<h2 id="categorical-data"><a class="header" href="#categorical-data">Categorical Data</a></h2>
<p>We define the <em>Categorical Data</em> as <em>Sparse Data</em> in our library. It is important to add the name to each <em>Sparse Data</em> for later embedding lookup. A <em>Categorical Data</em> is composed of a batch of 1d tensors of variable length.</p>
<p><em>code example</em></p>
<pre><code class="language-python">import numpy as np

from persia.prelude import PyPersiaBatchData

sparse_data_num = 3
batch_size = 1024
max_sparse_len = 65536

# gen mock sparse data
batch_sparse_datas = []
for feature_idx in range(sparse_data_num):
    batch_sparse_data = []
    for batch_idx in range(batch_size):
        cnt_sparse_len = np.random.randint(0, max_sparse_len)
        sparse_data = np.random.one((cnt_sparse_len), dtype=np.uint64)
    batch_sparse_datas.append((batch_sparse_data, f&quot;feature_{feature_idx}&quot;))

# add mock sparse data into PyPersiaBatchData 
batch_data = PyPersiaBatchData()
batch_data.add_sparse(batch_sparse_datas)
</code></pre>
<h2 id="label-data"><a class="header" href="#label-data">Label Data</a></h2>
<p>We use the <em>Target Data</em> to represent <em>Label</em>. <em>Target Data</em> should be a <code>2d flaot32 tensor</code>.</p>
<p><em>code example</em></p>
<pre><code class="language-python">import numpy as np

from persia.prelude import PyPersiaBatchData

batch_data = PyPersiaBatchData()
batch_size = 1024
batch_data.add_target(np.ones((1024, 2), dtype=np.float32))
</code></pre>
<footer id="open-on-gh">Found a bug? <a href="https://github.com/PersiaML/tutorials/edit/main/src/data-processing/index.md">Edit this file on GitHub.</a></footer><div style="break-before: page; page-break-before: always;"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css" integrity="sha384-AfEj0r4/OFrOo5t7NnNe46zW/tFgW6x/bCJG8FqQCEo3+Aro6EYUG4+cU+KJWu/X" crossorigin="anonymous">
<h1 id="monitoring"><a class="header" href="#monitoring">Monitoring</a></h1>
<p>Monitoring and alerting is crucial for a distributed system, PersiaML provides integration with <a href="https://prometheus.io/docs/introduction/overview/">Prometheus</a> for this purpose.</p>
<p>Services in PerisaML push their metrics to a <a href="https://github.com/prometheus/pushgateway">PushGateway</a>, the gateway then exposes these metrics to Prometheus.</p>
<h2 id="step-to-enable-metrics-in-perisaml"><a class="header" href="#step-to-enable-metrics-in-perisaml">Step to enable metrics in PerisaML</a></h2>
<ol>
<li>Enable metrics in configuration</li>
</ol>
<p>Add the following configurations in <a href="monitoring/../configuration/index.html"><code>global_config.yaml</code></a>.</p>
<p><code>job_name(str)</code> is a name to distinguish your job from others. It can be, for example, <code>dlrm_v1.0</code>.</p>
<pre><code class="language-yaml">PersiaMetricsConfig:
  enable_metrics: true
  job_name: your_job_name
</code></pre>
<ol start="2">
<li>Deploy PushGateway</li>
</ol>
<p>See <a href="https://github.com/prometheus/pushgateway">official documentation</a> for details. Here is an example for deploying gateway by <a href="https://docs.docker.com/compose/">docker-compose</a>.</p>
<p>The default push address on PersiaML services is <code>metrics_gateway:9091</code>, which can be override by the environment variable <code>PERSIA_METRICS_GATEWAY_ADDR</code>.</p>
<pre><code class="language-yaml">version: &quot;3.3&quot;
services:
    data_compose:
        ...

    trainer:
        ...

    middleware:
        ...

    server:
        ...

    persia_nats_service:
        ...

    metrics_gateway:
        image: prom/pushgateway:latest
        deploy:
            replicas: 1
</code></pre>
<p>You can test the metrics are there by doing:</p>
<pre><code class="language-bash">curl metrics_gateway:9091/metrics
</code></pre>
<p>in a service container.</p>
<ol start="3">
<li>Collecting metrics</li>
</ol>
<p>To collect metrics from the gateway, you need a prometheus service to do that for you.</p>
<p>Details of how to setup in various environments can be found in for example <a href="https://prometheus.io/docs/prometheus/latest/configuration/configuration/#docker_sd_config">docker_sd_config</a>, <a href="https://prometheus.io/docs/prometheus/latest/configuration/configuration/#kubernetes_sd_config">kubernetes_sd_config</a> or <a href="https://prometheus.io/docs/prometheus/latest/configuration/configuration/#dockerswarm_sd_config">dockerswarm_sd_config</a>.</p>
<h2 id="metrics-in-perisaml"><a class="header" href="#metrics-in-perisaml">Metrics in PerisaML</a></h2>
<ol>
<li>Accuracy related</li>
</ol>
<table><thead><tr><th>Key</th><th>Description</th></tr></thead><tbody>
<tr><td><code>index_miss_count</code></td><td>miss count of indices when lookup. There may be reasons for the missing of embeddings, e.g. lookup a new index or the index has been evicted.</td></tr>
<tr><td><code>index_miss_ratio</code></td><td>miss ratio of indices for all features when lookup for one batch.</td></tr>
<tr><td><code>gradient_id_miss_count</code></td><td>num of not found indices when updating gradient. This will happen when embedding evicted before update gradient only.</td></tr>
<tr><td><code>estimated_distinct_id</code></td><td>estimated distinct id for each feature.</td></tr>
<tr><td><code>batch_unique_indices_rate</code></td><td>unique indices rate in one batch.</td></tr>
<tr><td><code>staleness</code></td><td>staleness of sparse model. The iteration of dense model run one by one, while the embedding lookup happened before concurrently. The staleness describe the delay of embeddings. The value of staleness start with 0, increase one when lookup a batch, decrease one when a batch update its gradients</td></tr>
<tr><td><code>nan_grad_skipped</code></td><td>nan gradient count caused by dense part.</td></tr>
</tbody></table>
<ol start="2">
<li>Efficiency related</li>
</ol>
<table><thead><tr><th>Key</th><th>Description</th></tr></thead><tbody>
<tr><td><code>lookup_mixed_batch_time_cost</code></td><td>lookup embedding time cost on embedding server</td></tr>
<tr><td><code>num_pending_batches</code></td><td>num batches already sent to middleware but still waiting for trainer to trigger lookup. The pending batches will stored in forward buffer, which capacity is configurable by <a href="https://github.com/PersiaML/tutorials/blob/docs/monitoring/src/configuring/index.md#middleware_configs"><code>global_config.yaml</code></a>. Once the buffer full, middleware server may not accept new batches.</td></tr>
<tr><td><code>lookup_create_requests_time_cost</code></td><td>lookup preprocess time cost on middleware. Include ID hashing, dividing id accroding feature groups and embedding servers.</td></tr>
<tr><td><code>lookup_rpc_time_cost</code></td><td>lookup embedding time cost on middleware server for a batch, include lookup on embedding server and network transmission.</td></tr>
<tr><td><code>update_gradient_time_cost</code></td><td>update gradient time cost on middleware server for a batch.</td></tr>
<tr><td><code>summation_time_cost</code></td><td>lookup postprocess time cost on middleware, mainly is embedding summation.</td></tr>
<tr><td><code>lookup_batched_time_cost</code></td><td>lookup and pre/post process time cost on middleware server.</td></tr>
</tbody></table>
<footer id="open-on-gh">Found a bug? <a href="https://github.com/PersiaML/tutorials/edit/main/src/monitoring/index.md">Edit this file on GitHub.</a></footer><div style="break-before: page; page-break-before: always;"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css" integrity="sha384-AfEj0r4/OFrOo5t7NnNe46zW/tFgW6x/bCJG8FqQCEo3+Aro6EYUG4+cU+KJWu/X" crossorigin="anonymous">
<h1 id="inference"><a class="header" href="#inference">Inference</a></h1>
<p>To do inference for trained models, we need to deploy PersiaML middleware service, PersiaML embedding service, and <a href="https://github.com/pytorch/serve">TorchServe</a> services.</p>
<p>When a TorchServe inference server receives requests, it first looks up embeddings on PersiaML services, and then does the forward pass for the DNN part.</p>
<p><a href="https://github.com/pytorch/serve">TorchServe</a> is a flexible framework for serving PyTorch models. In this page, we will introduce how to deploy a PerisaML model with it.</p>
<p>In the following sections, we first introduce how to create a custom handler for TorchServe to query embeddings during inference. Next, we introduce how to save models during training and load models during inference. Then, we introduce how to deploy various services for inference. Finally, we introduce how to query the inference service to get the inference result.</p>
<h2 id="1-create-persiaml-handler-for-torchserve"><a class="header" href="#1-create-persiaml-handler-for-torchserve">1. Create PersiaML handler for TorchServe</a></h2>
<p>With TorchService, customized operations (like preprocess or postprocess) can be done with simple Python scripts, called <a href="https://github.com/pytorch/serve/blob/master/docs/custom_service.md#custom-handlers">custom handler</a>.</p>
<p>There are ways to write custom handler, one of them is <a href="https://github.com/pytorch/serve/blob/master/docs/custom_service.md#custom-handler-with-class-level-entry-point">custom-handler-with-class-level-entry-point</a>.</p>
<p>Here is an example to define a custom handler retrieving PersiaML embeddings:</p>
<pre><code class="language-python">from persia.ctx import InferCtx
from persia.prelude import forward_directly_from_bytes

from ts.torch_handler.base_handler import BaseHandler

from abc import ABC
import torch
import os

class PersiaHandler(BaseHandler, ABC):
    def initialize(self, context):
        super().initialize(context)
        self.persia_context = InferCtx()

    def preprocess(self, data):
        batch = data[0].get('batch')
        batch = bytes(batch)
        batch = forward_directly_from_bytes(batch, 0)

        model_input = self.persia_context.prepare_features(batch)
        return model_input

    def inference(self, data, *args, **kwargs):
        denses, sparses = data
        with torch.no_grad():
            results = self.model(denses, sparses)
        return results

    def postprocess(self, data):
        data = torch.reshape(data, (-1,))
        data = data.tolist()
        return [data]
</code></pre>
<h2 id="2-save-and-load-persiaml-model"><a class="header" href="#2-save-and-load-persiaml-model">2. Save and load PersiaML model</a></h2>
<p>The sparse part and the dense part of a PerisaML model are saved separately.</p>
<p>For the dense part, it is saved directly by PyTorch with <a href="https://pytorch.org/docs/stable/jit.html">TorchScript</a>:</p>
<pre><code class="language-python">jit_model = torch.jit.script(model)
jit_model.save('/your/model/dir/you_model_name.pth')
</code></pre>
<p>Then, to serve the dense part with TorchServe, use <a href="https://github.com/pytorch/serve/blob/master/model-archiver/README.md">torch-model-archiver</a> to package it.</p>
<pre><code class="language-bash">torch-model-archiver --model-name you_model_name --version 1.0 --serialized-file /your/model/dir/you_model_name.pth --handler /your/model/dir/persia_handler.py
</code></pre>
<p>Sparse model can be saved and loaded with PerisaML Python API, see <a href="inference/../model-checkpointing/index.html">Model Checkpointing</a> for details.</p>
<h2 id="3-deploy-perisaml-services-and-torchserve"><a class="header" href="#3-deploy-perisaml-services-and-torchserve">3. Deploy PerisaML services and TorchServe</a></h2>
<p>TorchServe can be launched with:</p>
<pre><code class="language-bash">torchserve --start --ncs --model-store /workspace/serve/model/ --models you_model_name.mar
</code></pre>
<p>There are configurations in <a href="https://github.com/PersiaML/tutorials/blob/docs/monitoring/src/configuring/index.md#global-config"><code>global_config.yaml</code></a> when deploy embedding servers and middleware for inference.</p>
<pre><code class="language-yaml">common_config:
  job_type: Infer
    servers:
      - emb_server_1:8000
      - emb_server_2:8000
    initial_sparse_checkpoint: /your/sparse/model/dir
</code></pre>
<h2 id="4-query-inference-result-with-grpc"><a class="header" href="#4-query-inference-result-with-grpc">4. Query inference result with gRPC</a></h2>
<p>There are ways to <a href="https://github.com/pytorch/serve#get-predictions-from-a-model">get predictions from a model</a> with TorchServe. One of them is using <a href="https://github.com/pytorch/serve#using-grpc-apis-through-python-client">gRPC API</a> through a <a href="https://github.com/pytorch/serve/blob/master/ts_scripts/torchserve_grpc_client.py">gRPC client</a>.</p>
<p>The input data is constructed in the same way as in training, Here is an example:</p>
<pre><code class="language-python">import grpc
import inference_pb2
import inference_pb2_grpc

def get_inference_stub():
    channel = grpc.insecure_channel('localhost:7070')
    stub = inference_pb2_grpc.InferenceAPIsServiceStub(channel)
    return stub

def infer(stub, model_name, model_input):
    with open(model_input, 'rb') as f:
        data = f.read()

    input_data = {'data': data}
    response = stub.Predictions(
        inference_pb2.PredictionsRequest(model_name=model_name, input=input_data))

    try:
        prediction = response.prediction.decode('utf-8')
        print(prediction)
    except grpc.RpcError as e:
        exit(1)

batch_size = 128
feature_dim = 16
denses = [np.random.rand(batch_size, 13).astype(np.float32)]
sparse = []
for sparse_idx in range(26):
    sparse.append((
        f'feature{sparse_idx + 1}',
        [np.random.randint(1000000, size=feature_dim).astype(np.uint64) for _ in range(batch_size)]
    ))

batch_data = PyPersiaBatchData()
batch_data.add_dense(denses)
batch_data.add_sparse(sparse)

model_input = batch_data.to_bytes()
infer(get_inference_stub(), 'you_model_name', model_input)
</code></pre>
<h2 id="5-model-incremental-update"><a class="header" href="#5-model-incremental-update">5. Model incremental update</a></h2>
<p>It is crucial to keep the model for inference up to date. For huge sparse models, PersiaML provides incremental updates, so that online prediction services only receives model differences during training to update the online model for inference. This dramatically reduces the model latency between training and inference.</p>
<p>During training, an incremental update file will be dumped periodically. During inference, PersiaML services keep scanning a directory to find if there is a new incremental update file to load.</p>
<p>Relavant configurations in <a href="https://github.com/PersiaML/tutorials/blob/docs/monitoring/src/configuring/index.md#global-config"><code>global_config.yaml</code></a> are <code>enable_incremental_update</code>, <code>incremental_buffer_size</code> and <code>incremental_dir</code>.</p>
<h2 id="6-manage-dense-models-on-torchserve"><a class="header" href="#6-manage-dense-models-on-torchserve">6. Manage dense models on TorchServe</a></h2>
<p>To update dense model with sparse model, it can be managed by torchserve through its <a href="https://github.com/pytorch/serve/blob/master/docs/management_api.md#management-api">management api</a>. After generating the <code>.mar</code> file according to the above steps, its path can be sent to torchserve with <a href="https://github.com/pytorch/serve/blob/master/ts_scripts/torchserve_grpc_client.py">grpc client</a>.</p>
<footer id="open-on-gh">Found a bug? <a href="https://github.com/PersiaML/tutorials/edit/main/src/inference/index.md">Edit this file on GitHub.</a></footer><div style="break-before: page; page-break-before: always;"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css" integrity="sha384-AfEj0r4/OFrOo5t7NnNe46zW/tFgW6x/bCJG8FqQCEo3+Aro6EYUG4+cU+KJWu/X" crossorigin="anonymous">
<h1 id="model-checkpointing"><a class="header" href="#model-checkpointing">Model Checkpointing</a></h1>
<p>A PerisaML model contains two parts: the dense part and the sparse part (embeddings). A checkpoint contains both the dense part and the sparse part can be saved together through Persia api or manually saved separately.</p>
<h2 id="checkpointing-together"><a class="header" href="#checkpointing-together">Checkpointing together</a></h2>
<p>We can call <code>load_checkpoint</code> or <code>dump_checkpoint</code> in a persia context, both the dense part and the sparse part will be saved into <code>checkpoint_dir</code>.</p>
<pre><code class="language-python">with TrainCtx(
    model=model,
    sparse_optimizer=sparse_optimizer,
    dense_optimizer=dense_optimizer,
    device_id=device_id,
    embedding_config=embedding_config,
) as ctx:
    ctx.load_checkpoint(checkpoint_dir)
    if batch_idx % 10000 == 0:
        ctx.dump_checkpoint(checkpoint_dir)
</code></pre>
<h2 id="checkpointing-separately"><a class="header" href="#checkpointing-separately">Checkpointing separately</a></h2>
<p>Since PyTorch is used for defining the dense part, it can be used directly for saving the dense part, see <a href="https://pytorch.org/tutorials/beginner/saving_loading_models.html">Saving and Loading Models</a>.</p>
<p>For the sparse part, we need to use PersiaML API to do model checkpointing.</p>
<p>In a persia context, we can load or dump the sparse part checkpoint in a directory with the <code>load_embedding</code>, <code>dump_embedding</code> method:</p>
<pre><code class="language-python">with TrainCtx(
    model=model,
    sparse_optimizer=sparse_optimizer,
    dense_optimizer=dense_optimizer,
    device_id=device_id,
    embedding_config=embedding_config,
) as ctx:
    ctx.load_embedding(checkpoint_dir, True)
    if batch_idx % 10000 == 0:
        ctx.dump_embedding(checkpoint_dir, True)
</code></pre>
<p>Relavant configurations in <a href="model-checkpointing/../configuration/index.html"><code>global_config.yaml</code></a> are <code>num_persistence_workers</code> and <code>num_signs_per_file</code>.</p>
<footer id="open-on-gh">Found a bug? <a href="https://github.com/PersiaML/tutorials/edit/main/src/model-checkpointing/index.md">Edit this file on GitHub.</a></footer><div style="break-before: page; page-break-before: always;"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css" integrity="sha384-AfEj0r4/OFrOo5t7NnNe46zW/tFgW6x/bCJG8FqQCEo3+Aro6EYUG4+cU+KJWu/X" crossorigin="anonymous">
<h1 id="configuration"><a class="header" href="#configuration">Configuration</a></h1>
<p>In order to achieve the best performance on various training and inference jobs, PersiaML servers provide a handful of configuration options via two config files, a global configuration file usually named as <code>global_config.yaml</code>, and an embedding configuration file usually named as <code>embedding_config.yaml</code>. The global configuration allows one to define job type and general behaviors of servers, whereas embedding configuration provides definition of embedding details for individual sparse features.</p>
<h2 id="global-configuration"><a class="header" href="#global-configuration">Global Configuration</a></h2>
<p>Global configuration specifies the configuration of the current PersiaML job. The path to the global configuration file should be parsed as argument <code>--global-config</code> when launching PersiaML servers or middleware.</p>
<p>Here is an example for <code>global_config.yaml</code>.</p>
<pre><code class="language-yaml">common_config:
  job_type: Train
  metrics_config:
    enable_metrics: false
    push_interval_sec: 10
embedding_server_config:
  capacity: 100000000
  num_hashmap_internal_shards: 128
  full_amount_manager_buffer_size: 1000
  num_persistence_workers: 4
  num_signs_per_file: 5000000
  enable_incremental_update: false
  incremental_buffer_size: 5000000
  incremental_channel_capacity: 1000
middleware_config:
  forward_buffer_size: 1000
</code></pre>
<p>Depending on the scope, <code>global_config</code> was divided into three major sections, namely <code>common_config</code>, <code>embedding_server_config</code> and <code>middleware_config</code>. <code>common_config</code> configures the job type (<code>job_type</code>) and metrics server. <code>embedding_server_config</code> configures the PersiaML embedding server, and <code>middleware_config</code> provides configurations for the PersiaML middleware. The following is a detailed description of each configuration.</p>
<h3 id="common_config"><a class="header" href="#common_config">common_config</a></h3>
<h4 id="job_type"><a class="header" href="#job_type">job_type</a></h4>
<p>The job_type of PresiaML can be either <code>Train</code> or <code>Infer</code>.</p>
<p>When <code>job_type</code> is <code>Infer</code>, additional configurations including <code>servers</code> and <code>initial_sparse_checkpoint</code> have to be provided. Here is an example:</p>
<pre><code class="language-yaml">common_config:
  job_type: Infer
    servers:
      - emb_server_1:8000
      - emb_server_2:8000
    initial_sparse_checkpoint: /your/sparse/model/dir
</code></pre>
<ul>
<li><code>servers(list of str, required)</code>: list of embedding servers each in the <code>ip:port</code> format.</li>
<li><code>initial_sparse_checkpoint(str, required)</code>: Embedding server will load this ckpt when start.</li>
</ul>
<h4 id="metrics_config"><a class="header" href="#metrics_config">metrics_config</a></h4>
<p><code>metrics_config</code> defines a set of configuration options for monitoring. See <a href="configuration/../monitoring/index.html">Monitoring</a> for details.</p>
<ul>
<li><code>enable_metrics(bool, default=false)</code>: Whether to enable metrics.</li>
<li><code>push_interval_sec(int ,default=10)</code>: The interval of pushing metrics to the promethus pushgateway server.</li>
<li><code>job_name(str, default=persia_defalut_job_name)</code>: A name to distinguish your job from others.</li>
</ul>
<h3 id="embedding_server_config"><a class="header" href="#embedding_server_config">embedding_server_config</a></h3>
<p><code>embedding_server_config</code> specifies the configuration for the embedding server.</p>
<ul>
<li><code>capacity(int, default=1,000,000,000)</code>: The capacity of each embedding server. Once the number of indices of an embedding server exceeds the capacity, it will evict embeddings according to <a href="https://en.wikipedia.org/wiki/Cache_replacement_policies#Least_recently_used_(LRU)">LRU</a> policies.</li>
<li><code>num_hashmap_internal_shards(int, default=100)</code>: The number of internal shard of an embedding server. Embeddings are saved in a HashMap which contains multiple shards (sub-hashmaps). Since the CRUD operations need to acquire the lock of a hashmap, acquiring the lock of the sub-hashmap instead of the whole hashmap will be more conducive to concurrency between CRUD operations.</li>
<li><code>full_amount_manager_buffer_size(int, default=1000)</code>: The buffer size of full amount manager. In order to achieve better performance, the embedding server does not traverse the hashmap directly during full dump. Instead, Embedding is submitted asynchronously through full amount manager.</li>
<li><code>num_persistence_workers(int, default=4)</code>: The concurrency of embedding dumping, loading and incremental update.</li>
<li><code>num_signs_per_file(int, default=1,000,000)</code>: Number of embeddings to be saved in each file in the checkpoint directory.</li>
<li><code>enable_incremental_update(bool, default=false)</code>: Whether to enable incremental update.</li>
<li><code>incremental_buffer_size(int, default=1,000,000)</code>: Buffer size for incremental update. Embeddings will be inserted into this buffer after each gradient update, and will only be dumped when the buffer is full. Only valid when <code>enable_incremental_update=true</code>.</li>
<li><code>incremental_dir(str, default=/workspace/incremental_dir/)</code>: The directory for incremental update files to be dumped or loaded.</li>
</ul>
<h3 id="middleware_configs"><a class="header" href="#middleware_configs">middleware_configs</a></h3>
<ul>
<li><code>forward_buffer_size(int, default=1000)</code>: Buffer size for prefoard batch data from data loader.</li>
</ul>
<h2 id="embedding-config"><a class="header" href="#embedding-config">Embedding Config</a></h2>
<p>In addition to <code>global_config</code>, detailed settings related to sparse feature embeddings are provided in a separate embedding configuration file usually named <code>embedding_config.yaml</code>. The path to the embedding config file should be parsed as argument <code>--embedding-config</code> when running PersiaML servers.</p>
<p>Here is an example for <code>embedding_config.yaml</code>.</p>
<pre><code class="language-yaml">feature_index_prefix_bit: 8
slot_config:
  workclass:
    dim: 8
    embedding_summation: true
  education:
    dim: 8
    embedding_summation: true
  marital_status:
    dim: 8
    embedding_summation: true
  occupation:
    dim: 8
    embedding_summation: true
  relationship:
    dim: 8
    embedding_summation: true
  race:
    dim: 8
    embedding_summation: true
  gender:
    dim: 8
    embedding_summation: true
  native_country:
    dim: 8
    embedding_summation: true
feature_groups:
  group1:
    - workclass
    - education
    - race
  group2:
    - marital_status
    - occupation

</code></pre>
<p>The following is a detailed description of each configuration. <code>required</code> means there are no default values.</p>
<ul>
<li>
<p><code>feature_index_prefix_bit(int, default=8)</code>: Number of bits occupied by each feature group. To avoid hash collisions between different features, the first <code>n(n=feature_index_prefix_bit)</code> bits of an index(u64) are taken as the feature bits, and the last <code>64-n</code> bits are taken as the index bits. The original id will be processed before inserted into the hash table, following <code>ID = original_ID % 0~2^(64-n) + index_prefix &lt;&lt; (64-n)</code>. Slots in the same feature group share the same <code>index_prefix</code>, which is automatically generated by Persia according to the <code>feature_groups</code>.</p>
</li>
<li>
<p><code>slots_config(map, required)</code>: <code>slots_config</code> contains all the definitions of Embedding. The key of the map is the feature name, and the value of the map is a struct named <code>SlotConfig</code>. The following is a detailed description of configuration in a <code>SlotConfig</code>.</p>
<ul>
<li><code>dim(int, required)</code>: dim of embedding.</li>
<li><code>sample_fixed_size(int, default=10)</code>: raw embedding placeholder size to fill 3d tensor -&gt; (bs, sample_fix_sized, dim).</li>
<li><code>embedding_summation(bool, default=true)</code>: whether to reduce(summation) embedding before feeding to dense net.</li>
<li><code>sqrt_scaling(bool, default=false)</code>: whether to numerical scaling embedding values.</li>
<li><code>hash_stack_config(struct, default=None)</code>: a method to represent a large number of sparse features with a small amount of Embedding vector. It means mapping the original ID to <code>0~E (E=embedding_size)</code> through <code>n (n=hash_stack_rounds)</code> different hash functions, such as <code>ID_1, ID_2... ID_n</code>. Each such ID corresponds to an embedding vector, then performs reduce(summation) operation among these embedding vectors, as input to the dense net of the original ID.
<ul>
<li><code>hash_stack_rounds(int, default=0)</code>: Embedding hash rounds.</li>
<li><code>embedding_size(int, default=0)</code>: Embedding hash space of each rounds.</li>
</ul>
</li>
</ul>
</li>
<li>
<p><code>feature_groups(map, default={})</code>: Feature group division. Refer to the description of <code>feature_index_prefix_bit</code>. Feature in one feature group will share the same index prefix.</p>
</li>
</ul>
<footer id="open-on-gh">Found a bug? <a href="https://github.com/PersiaML/tutorials/edit/main/src/configuration/index.md">Edit this file on GitHub.</a></footer><div style="break-before: page; page-break-before: always;"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css" integrity="sha384-AfEj0r4/OFrOo5t7NnNe46zW/tFgW6x/bCJG8FqQCEo3+Aro6EYUG4+cU+KJWu/X" crossorigin="anonymous">
<h1 id="troubleshooting"><a class="header" href="#troubleshooting">Troubleshooting</a></h1>
<p>Debugging running task in distributed training can be difficult.We provide some general suggestions in this page.</p>
<h2 id="investigate-debug-log"><a class="header" href="#investigate-debug-log">Investigate debug log</a></h2>
<p>PersiaML print the log according to different <code>LOG_LEVEL</code>.Set corresponding launch environment <code>LOG_LEVEL</code> when launch the <em>trainer</em>, <em>middleware-server</em> and <em>embedding-server</em>. The value of <em>LOG_LEVEL</em> can be accepted include <em>debug</em>, <em>info</em>, <em>warn</em> and <em>error</em> can be accept, the default value is <code>info</code>.</p>
<h2 id="investigate-grafana-metrics"><a class="header" href="#investigate-grafana-metrics">Investigate Grafana metrics</a></h2>
<p>We use the <code>Prometheus</code> to collect the metrics in training phase.You can find out the information such as current embedding staleness, current embedding_size or the time cost of embedding backward. Read the <a href="troubleshooting/../monitoring/index.html">Monitoring cheapter</a> for more metric.</p>
<h2 id="print-intermediate-results"><a class="header" href="#print-intermediate-results">Print intermediate results</a></h2>
<p>Print the intermediate result is also necessary when meet some tough problems.When you need to print the intermediate results during the training phase, there are two solutions that you can do to after adding the intermediate log.The one is building a new docker image.And the second is install the persiaML python library manully. Read the <a href="troubleshooting/../installation/index.html">Installation cheapter</a> for more detail.</p>
<footer id="open-on-gh">Found a bug? <a href="https://github.com/PersiaML/tutorials/edit/main/src/troubleshooting/index.md">Edit this file on GitHub.</a></footer><div style="break-before: page; page-break-before: always;"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css" integrity="sha384-AfEj0r4/OFrOo5t7NnNe46zW/tFgW6x/bCJG8FqQCEo3+Aro6EYUG4+cU+KJWu/X" crossorigin="anonymous">
<h1 id="kubernetes-integration"><a class="header" href="#kubernetes-integration">Kubernetes Integration</a></h1>
<footer id="open-on-gh">Found a bug? <a href="https://github.com/PersiaML/tutorials/edit/main/src/kubernetes-integration/index.md">Edit this file on GitHub.</a></footer>
                    </main>

                    <nav class="nav-wrapper" aria-label="Page navigation">
                        <!-- Mobile navigation buttons -->
                        <div style="clear: both"></div>
                    </nav>
                </div>
            </div>

            <nav class="nav-wide-wrapper" aria-label="Page navigation">
            </nav>

        </div>

        <script type="text/javascript">
            window.playground_copyable = true;
        </script>
        <script src="elasticlunr.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="mark.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="searcher.js" type="text/javascript" charset="utf-8"></script>
        <script src="clipboard.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="highlight.js" type="text/javascript" charset="utf-8"></script>
        <script src="book.js" type="text/javascript" charset="utf-8"></script>

        <!-- Custom JS scripts -->
        <script type="text/javascript">
        window.addEventListener('load', function() {
            window.setTimeout(window.print, 100);
        });
        </script>
    </body>
</html>
